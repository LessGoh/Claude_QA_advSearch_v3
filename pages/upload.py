import streamlit as st
import tempfile
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from utils.document_manager import DocumentManager
from utils.auth import get_user_index_id
from utils.enhanced_document_processor import EnhancedDocumentProcessor
from utils.config import get_api_keys
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

class UploadPage:
    def __init__(self):
        self.doc_manager = DocumentManager()
        
        # Initialize enhanced processor
        openai_key, pinecone_key, _ = get_api_keys()
        if openai_key and pinecone_key:
            self.enhanced_processor = EnhancedDocumentProcessor(openai_key, pinecone_key)
        else:
            self.enhanced_processor = None
    
    def render(self):
        st.markdown("# üìé –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # Check if enhanced processor is available
        if not self.enhanced_processor:
            st.error("üîß –°–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API –∫–ª—é—á–µ–π.")
            return
        
        # Check if vectorstore is available
        if not st.session_state.index_manager:
            st.error("–ú–µ–Ω–µ–¥–∂–µ—Ä –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return
        
        # Current index info
        user_id = get_user_index_id(st.session_state.username)
        current_index_name = st.session_state.current_index or f"pdf-qa-personal-{user_id}"
        index_type = "–õ–∏—á–Ω—ã–π" if current_index_name.endswith(f"-{user_id}") else "–û–±—â–∏–π"
        
        st.info(f"üìä **–ó–∞–≥—Ä—É–∑–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å:** {index_type}")
        
        # Index selection
        st.markdown("## üéØ –í—ã–±–æ—Ä –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        col1, col2 = st.columns(2)
        with col1:
            target_index = st.radio(
                "–ö—É–¥–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã:",
                ["–õ–∏—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å", "–û–±—â–∏–π –∏–Ω–¥–µ–∫—Å"],
                index=0 if index_type == "–õ–∏—á–Ω—ã–π" else 1,
                help="–õ–∏—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å - —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∞—Å, –û–±—â–∏–π - –¥–ª—è –≤—Å–µ–π –∫–æ–º–∞–Ω–¥—ã"
            )
        
        with col2:
            if target_index == "–õ–∏—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å":
                st.info("üîí –î–æ–∫—É–º–µ–Ω—Ç—ã –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ –≤–∞–º")
            else:
                st.info("üë• –î–æ–∫—É–º–µ–Ω—Ç—ã –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤—Å–µ–π –∫–æ–º–∞–Ω–¥–µ")
        
        # File upload section
        st.markdown("## üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
        
        # Enhanced features info
        st.info("üöÄ **–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ v3:**\n"
                "‚Ä¢ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞\n"
                "‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª –∏ —Ç–∞–±–ª–∏—Ü\n"
                "‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\n"
                "‚Ä¢ –¢–æ—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        # Drag and drop area
        uploaded_files = st.file_uploader(
            "–ü–µ—Ä–µ—Ç–∞—â–∏—Ç–µ —Ñ–∞–π–ª—ã —Å—é–¥–∞ –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ –¥–ª—è –≤—ã–±–æ—Ä–∞",
            type="pdf",
            accept_multiple_files=True,
            help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF —Ñ–∞–π–ª—ã. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 50MB –Ω–∞ —Ñ–∞–π–ª"
        )
        
        if uploaded_files:
            self._display_file_list(uploaded_files)
            
            # Process button
            col1, col2 = st.columns([3, 1])
            with col1:
                use_enhanced = st.checkbox("üî¨ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É v3", 
                                         value=True,
                                         help="–í–∫–ª—é—á–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —É–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∏ —Ç–æ—á–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã")
            with col2:
                if st.button("üöÄ –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã", use_container_width=True):
                    self._process_files(uploaded_files, target_index, use_enhanced)
    
    def _display_file_list(self, uploaded_files):
        """Display list of uploaded files with validation"""
        st.markdown("### üìã –í—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        
        MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
        valid_files = []
        invalid_files = []
        
        for i, file in enumerate(uploaded_files):
            file_size_mb = file.size / (1024 * 1024)
            
            if file.size > MAX_FILE_SIZE:
                invalid_files.append(f"{file.name} ({file_size_mb:.1f}MB - —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)")
                st.error(f"‚ùå {file.name} ({file_size_mb:.1f}MB) - –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞")
            else:
                valid_files.append(file)
                st.success(f"‚úÖ {file.name} ({file_size_mb:.1f}MB)")
        
        if invalid_files:
            st.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ (50MB)")
        
        return valid_files
    
    def _process_files(self, uploaded_files, target_index, use_enhanced=True):
        """Process uploaded files with optional enhanced processing"""
        # Filter valid files
        MAX_FILE_SIZE = 50 * 1024 * 1024
        valid_files = [f for f in uploaded_files if f.size <= MAX_FILE_SIZE]
        
        if not valid_files:
            st.error("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        # Get appropriate vectorstore
        if target_index == "–õ–∏—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å":
            vectorstore = st.session_state.index_manager.get_vectorstore("personal", st.session_state.username)
            user_id = get_user_index_id(st.session_state.username)
            index_name = f"pdf-qa-personal-{user_id}"
        else:
            vectorstore = st.session_state.index_manager.get_vectorstore("shared")
            index_name = "pdf-qa-shared"
        
        if not vectorstore:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            return
        
        # Process files
        if use_enhanced and self.enhanced_processor:
            results = self._process_multiple_pdfs_enhanced(valid_files, index_name)
        else:
            results = self._process_multiple_pdfs_legacy(valid_files, vectorstore, index_name)
        self._display_results(results)
    
    def _process_multiple_pdfs_enhanced(self, uploaded_files, index_name):
        """Process multiple PDF files with enhanced v3 processing"""
        def process_single_file_enhanced(file_data):
            file, index_name, username = file_data
            start_time = time.time()
            
            try:
                # Create temporary file
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(file.getvalue())
                    tmp_file_path = tmp_file.name
                
                # Process with enhanced processor
                result = self.enhanced_processor.process_document(
                    tmp_file_path, 
                    index_name,
                    document_id=None
                )
                
                # Clean up temporary file
                os.unlink(tmp_file_path)
                
                # Store enhanced document metadata
                if result['status'] == 'success':
                    doc_metadata = {
                        'filename': file.name,
                        'upload_user': username,
                        'upload_date': datetime.now().isoformat(),
                        'file_size': file.size,
                        'chunk_count': result['chunks_created'],
                        'sections_detected': result['sections_detected'],
                        'has_formulas': result['has_formulas'],
                        'has_tables': result['has_tables'],
                        'index_name': index_name,
                        'status': 'success',
                        'document_id': result['document_id'],
                        'document_title': result['document_title'],
                        'processing_method': 'enhanced_v3'
                    }
                    
                    # Save to document manager
                    self.doc_manager.add_document(doc_metadata)
                
                processing_time = time.time() - start_time
                return {
                    "file": file.name,
                    "status": result['status'], 
                    "chunks": result.get('chunks_created', 0),
                    "sections": result.get('sections_detected', 0),
                    "has_formulas": result.get('has_formulas', False),
                    "has_tables": result.get('has_tables', False),
                    "document_title": result.get('document_title', file.name),
                    "processing_time": processing_time,
                    "processing_method": "enhanced_v3"
                }
                
            except Exception as e:
                # Clean up temporary file if it exists
                try:
                    if 'tmp_file_path' in locals():
                        os.unlink(tmp_file_path)
                except:
                    pass
                
                processing_time = time.time() - start_time
                return {
                    "file": file.name,
                    "status": "error",
                    "error": str(e),
                    "chunks": 0,
                    "processing_time": processing_time,
                    "processing_method": "enhanced_v3"
                }
        
        # Process files with progress tracking
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Use ThreadPoolExecutor for parallel processing
        with ThreadPoolExecutor(max_workers=2) as executor:  # Reduced workers for enhanced processing
            # Prepare file data for processing
            file_data = [(file, index_name, st.session_state.username) for file in uploaded_files]
            
            # Submit all tasks
            future_to_file = {
                executor.submit(process_single_file_enhanced, data): data[0].name 
                for data in file_data
            }
            
            # Process completed tasks
            completed = 0
            total_files = len(uploaded_files)
            
            for future in as_completed(future_to_file):
                filename = future_to_file[future]
                completed += 1
                
                # Update progress
                progress = completed / total_files
                progress_bar.progress(progress)
                status_text.text(f"–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {filename}... ({completed}/{total_files})")
                
                # Get result
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    results.append({
                        "file": filename,
                        "status": "error",
                        "error": str(e),
                        "chunks": 0,
                        "processing_method": "enhanced_v3"
                    })
        
        # Clear progress indicators
        progress_bar.empty()
        status_text.empty()
        
        return results
    
    def _process_multiple_pdfs_legacy(self, uploaded_files, vectorstore, index_name):
        """Process multiple PDF files with legacy processing"""
        def process_single_file(file_data):
            file, vectorstore, index_name, username = file_data
            start_time = time.time()
            
            try:
                # Create temporary file
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(file.getvalue())
                    tmp_file_path = tmp_file.name
                
                # Load and process PDF
                pdf_loader = PyPDFLoader(tmp_file_path)
                pages = pdf_loader.load_and_split()
                
                # Split into chunks
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000, 
                    chunk_overlap=100
                )
                splits = text_splitter.split_documents(pages)
                
                # Add metadata
                for split in splits:
                    split.metadata.update({
                        'filename': file.name,
                        'upload_user': username,
                        'upload_date': datetime.now().isoformat(),
                        'file_size': file.size,
                        'index_name': index_name
                    })
                
                # Add documents to vector store
                vectorstore.add_documents(documents=splits)
                
                # Clean up temporary file
                os.unlink(tmp_file_path)
                
                # Store document metadata
                doc_metadata = {
                    'filename': file.name,
                    'upload_user': username,
                    'upload_date': datetime.now().isoformat(),
                    'file_size': file.size,
                    'chunk_count': len(splits),
                    'index_name': index_name,
                    'status': 'success'
                }
                
                # Save to document manager
                self.doc_manager.add_document(doc_metadata)
                
                processing_time = time.time() - start_time
                return {
                    "file": file.name,
                    "status": "success", 
                    "chunks": len(splits),
                    "processing_time": processing_time,
                    "processing_method": "legacy"
                }
                
            except Exception as e:
                # Clean up temporary file if it exists
                try:
                    if 'tmp_file_path' in locals():
                        os.unlink(tmp_file_path)
                except:
                    pass
                
                processing_time = time.time() - start_time
                return {
                    "file": file.name,
                    "status": "error",
                    "error": str(e),
                    "chunks": 0,
                    "processing_time": processing_time,
                    "processing_method": "legacy"
                }
        
        # Process files with progress tracking
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Use ThreadPoolExecutor for parallel processing
        with ThreadPoolExecutor(max_workers=3) as executor:
            # Prepare file data for processing
            file_data = [(file, vectorstore, index_name, st.session_state.username) for file in uploaded_files]
            
            # Submit all tasks
            future_to_file = {
                executor.submit(process_single_file, data): data[0].name 
                for data in file_data
            }
            
            # Process completed tasks
            completed = 0
            total_files = len(uploaded_files)
            
            for future in as_completed(future_to_file):
                filename = future_to_file[future]
                completed += 1
                
                # Update progress
                progress = completed / total_files
                progress_bar.progress(progress)
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {filename}... ({completed}/{total_files})")
                
                # Get result
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    results.append({
                        "file": filename,
                        "status": "error",
                        "error": str(e),
                        "chunks": 0
                    })
        
        # Clear progress indicators
        progress_bar.empty()
        status_text.empty()
        
        return results
    
    def _display_results(self, results):
        """Display processing results with enhanced information"""
        successful_files = [r for r in results if r["status"] == "success"]
        failed_files = [r for r in results if r["status"] == "error"]
        enhanced_files = [r for r in successful_files if r.get("processing_method") == "enhanced_v3"]
        
        if successful_files:
            total_chunks = sum(r["chunks"] for r in successful_files)
            total_time = sum(r["processing_time"] for r in successful_files)
            
            # Enhanced statistics
            if enhanced_files:
                total_sections = sum(r.get("sections", 0) for r in enhanced_files)
                formula_docs = len([r for r in enhanced_files if r.get("has_formulas", False)])
                table_docs = len([r for r in enhanced_files if r.get("has_tables", False)])
                
                st.success(f"‚úÖ {len(successful_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ! "
                          f"–°–æ–∑–¥–∞–Ω–æ {total_chunks} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–∞ {total_time:.1f}—Å.")
                
                if enhanced_files:
                    st.info(f"üî¨ **–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (v3):** {len(enhanced_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"
                           f"‚Ä¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {total_sections}\n"
                           f"‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ñ–æ—Ä–º—É–ª–∞–º–∏: {formula_docs}\n"
                           f"‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏: {table_docs}")
            else:
                st.success(f"‚úÖ {len(successful_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ! –°–æ–∑–¥–∞–Ω–æ {total_chunks} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–∞ {total_time:.1f}—Å.")
            
            # Show successful files with detailed info
            with st.expander("üìã –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"):
                for result in successful_files:
                    method = result.get("processing_method", "legacy")
                    
                    if method == "enhanced_v3":
                        # Enhanced processing result
                        extras = []
                        if result.get("sections", 0) > 0:
                            extras.append(f"{result['sections']} —Ä–∞–∑–¥–µ–ª–æ–≤")
                        if result.get("has_formulas", False):
                            extras.append("—Ñ–æ—Ä–º—É–ª—ã")
                        if result.get("has_tables", False):
                            extras.append("—Ç–∞–±–ª–∏—Ü—ã")
                        
                        extra_info = f" ({', '.join(extras)})" if extras else ""
                        title = result.get("document_title", result["file"])
                        
                        st.write(f"‚úÖüî¨ **{result['file']}** - {title}")
                        st.write(f"    ‚îî {result['chunks']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤{extra_info} ({result['processing_time']:.1f}—Å)")
                    else:
                        # Legacy processing result
                        st.write(f"‚úÖ {result['file']} - {result['chunks']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ ({result['processing_time']:.1f}—Å)")
        
        if failed_files:
            st.error(f"‚ùå {len(failed_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å")
            # Show failed files
            with st.expander("‚ö†Ô∏è –û—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"):
                for result in failed_files:
                    method = result.get("processing_method", "unknown")
                    st.write(f"‚ùå {result['file']} ({method}) - {result['error']}")

def render_upload_page():
    """Render the upload page"""
    upload_page = UploadPage()
    upload_page.render()